---
title: "Multiverse analysis study - Tutorial"
output:
  html_document:
    df_print: paged
---

```{r setup}
if (!require(ggplot2)) {
  # Installs ggplot2 if it is missing
  # The installation process may ask you to update other packages. 
  # If asked, select the option to update All.
  install.packages("ggplot2")
  library(ggplot2)
}
```

## Introduction

Data analyses for any scientific research in an iterative, multi-stage process.
The statistical results that are reported in a published paper are usually one
of many reasonable analyses arising from the iterative process. A multiverse
analysis aims to increase transparency by performing multiple analyses on a
given dataset based on a set of defensible analysis procedures. In this 
tutorial, we will look at a simple example and you will complete extend the
example in the following activity.

## Learning objectives

In this tutorial, you will practice defining and exploring a set of multiple
analyses for a given dataset in R. You will:

+ define multiple models using purpose-built package, `mverse`,
+ fit the models using `glm_mverse()`,
+ extract and save quantities of interest from the fitted models, and
+ organize and visualize the extracted quantities to help answer a research 
  question.

## Data

We are given a dataset that contains information on mortgage applications made 
in 1990 in Boston. We can read the dataset and store it in `hdma` as shown 
below. You can run the code below by clicking the green arrow button on the
right of the code chunk or pressing 'Ctrl + Shift + Enter' with your cursor 
inside the code chunk to view the dataset.

```{r}
# the dataset is stored in the file named hdma.csv
hdma <- read.csv("hdma.csv")
hdma 
```

Each row of the dataset represents a mortgage application with the following
information in the columns:

+  `is_approved` is 1 if the application was approved and 0 otherwise.
+  `is_female` is 1 if the applicant was a female and 0 otherwise.
+  `is_black` is 1 if the applicant was a black or Hispanic and 0 if the 
applicant was non-Hispanic white. The dataset does not contain other races.
+  `is_married` is 1 if the applicant was married and 0 otherwise.
+  `is_housing_expense_ratio_high` is 1 if the bank's calculation of housing
expense over income exceeds 30% and 0 otherwise.
+  `is_self_employed` is 1 if the applicant was self-employed and 0 otherwise.
+  `is_bad_credit` is 1 if the applicant had one or more public records such as
bankruptcy, charge offs, and collection actions and 0 otherwise.
+  `payment_income_ratio` is the bank's calculation of total debt payment over
income in percentages.
+  `loan_to_value_ratio` is the value of the loan amount over the appraisal
value of the property in percentages.

## Research question

We are interested in answering the following research question.

> Did mortgage providers approve an application differently based on the 
applicant's sex in Boston in 1990?

To answer the question, we can conduct a hypothesis test with the following
set of hypotheses.

> - $H_0$: No, they were as likely to approve female applicants as male 
applicants.
> - $H_1$: Yes, they were either more likely or less likely to approve female
applicants than male applicants.

We will conduct the test at 5% significance level, or 95% confidence level.

_Recall that a hypothesis test investigates whether a given data set provides
evidence against the null hypothesis, $H_0$, which leads to the that the 
alternative hypothesis, $H_1$ is true._


## A simple regression model

One way to conduct the hypothesis test for the given research question is 
checking whether the 95% confidence interval for the coefficient for the term
`is_female` includes 0 in a simple regression model that describes the 
relationship between the explanatory variable `is_female` and the response 
variable `is_approved`. In R, we can write the formula for defining the
relationship as `is_approved ~ is_female`.

__Run__ the code below. This defines and stores the formula in `simple_formula`.

```{r}
simple_formula <- "is_approved ~ is_female"
```

A linear regression model using ordinary least squares is not appropriate for
modelling the relationship since `is_approved` is a binary variable and it is
not reasonable to assume a normal distribution for the variable. Instead, we
describe the relationship using logistic regression which assumes a binomial
distribution for the response variable. 

_We won't go into details about how logistic regression works. In this tutorial,
we will focus on testing whether a coefficient in a logistic regression model is
zero or not using confidence intervals._

In R, you can use the function `glm()` with the option `family = binomial` to
fit a logistic regression model. The function fits a family of generalized
linear regression models and `family = binomial` specifies the underlying
assumption about the response variable's distribution. We can then extract the 
appropriate quantities from the fitted model as needed.

__Run__ each of the code chunks below to fit the logistic regression model, and
to extract the coefficient estimate and confidence intervals for `is_female`.

1. Fit the logistic regression model using `glm()`.  
+ `formula = siimple_formula` specifies the model formula
+ `data = hdma` specifies the dataset used to fit the model

```{r}
fit <- glm(formula = simple_formula, data = hdma, family = binomial)
```

2. Extract the coefficient estimates from the fitted model using `coef()`. To 
extract the coefficient for `is_female` only we can find the element with 
matching name using `names()`.

```{r}
ests <- coef(fit)
ests # note the named vector.
est_is_female <- ests[names(ests) == "is_female"]
est_is_female
```

The estimate represents the log-odd ratio for mortgage approval between female
and male applicants. Exponentiating the value retrieves the odd ratio.

```{r}
exp(est_is_female)
```

An odd of an event is defined as the probability of the event over the
probability of the complement. In our problem, the odd of a mortgage approval
is the probability of a mortgage being approved over the probability of the
mortgage being rejected. The ratio estimated by `exp(est_is_female)` is the
ratio of the odd for female applicants over the odd for male applicants. 

3. Extract the confidence intervals from the fitted model using `confint()`. The
function outputs a matrix of, by default, 95%, confidence intervals for all
terms. We can extract the interval for `is_female` only by matching the row name
using `rownames()`.

```{r}
cis <- confint(fit)
cis # note the matrix with named rows and columns
ci_is_female <- cis[rownames(cis) == "is_female", ]
ci_is_female
```

_Aside: There is more than one way to compute confidence intervals for models
fitted with `glm()`. `confint()` returns a type of confidence intervals called
profile confidence intervals._

By printing `ci_is_female`, we can see that the 95% confidence interval for the
`is_female` coefficient contains 0. Based on this result, we fail to reject the
null hypothesis. 

> Using the the logistic regression model with only a single explanatory
variable, we do not find statistically significant evidence that an applicant's
sex was relevant in the probability of approval for their mortgage application.


### _Practice_

Mortgage providers consider the applicant's financial information when assessing
an application. One may argue that the analysis above doesn't consider any
financial information and that `payment_income_ratio` must be included as an
explanatory variable in addition to `is_female`, or a covariate, to answer the
research question.

In the code chunk provided below, fit the multiple logistic regression model
with `is_approved` as the response variable and `is_female` and
`payment_income_ratio` as the explanatory variables. From the fitted model,
extract the 95% confidence interval for the `is_female` variable.

_Hint: To specify multiple explanatory variables in a model formula, you can 
"add" the variable on the righ-hand side. e.g., `y ~ x + w + z`_

```{r}
# write your code here

```

## Multiverse analysis 

The dataset includes more than the payment to income ratio for each application.
It includes other information about the applicant's financial information as
well as other demographic information. The mortgage providers also had access
to them when making their decisions. However, it's difficult to say that all of
the information were relevant when making the decisions on the approvals. 

Assume that any combination of the extra variables, or covariates, included in 
the dataset makes a defensible model for answering the research question. A 
multiverse analysis analyzes and reports results from all of the defensible 
models. For this tutorial, we will only consider `payment_income_ratio` and 
`is_married` as covariates.

### Defining the multiverse

With the 2 covariates, we can construct 4 defensible models in the following 
ways:

1. Do not include either `payment_income_ratio` or `is_married`
2. Include only `payment_income_ratio`
3. Include only `is_married`
4. Include both `payment_income_ratio` and `is_married`

> We will use the package `mverse`. The package is designed specifically to help
define and explore multiverse analysis easily. 

__Run__ each of the code chunks below to define the multiverse with the package.

1. Install and load the package.

```{r}
# The installation process may ask you to update other packages. 
# Select the option to update All.
remotes::install_github("mverseanalysis/mverse")
library(mverse)
```

2. Define the decision branch. In `mverse`, each decision point is called a 
branch. `formula_branch()` defines options around the model formula. 

You can specify the 4 models manually by typing all formulae in the function:

```{r}
formulae <- formula_branch(
  is_approved ~ is_female, 
  is_approved ~ is_female + payment_income_ratio,
  is_approved ~ is_female + is_married,
  is_approved ~ is_female + payment_income_ration + is_married
  )
formulae
```

Or, you can pass the two covariates to the optional argument `covariate = ` to
specify that they are optional in forming the model formula and all combinations
need to be used.

```{r}
formulae <- formula_branch(
  is_approved ~ is_female, 
  covariates = c("payment_income_ratio", "is_married")
  )
formulae
```

3. Define a multiverse based on the branch. In `mverse`, any multiverse analysis
is defined by a source dataset and a set of branches. We can make use of R's
pipe operator, `|>`, to create a multiverse analysis object with the source
dataset and consecutively add the formula branch defined above.

```{r}
mv <- create_multiverse(hdma) |>
  add_formula_branch(formulae)
summary(mv)
```

_Note that all 4 models include `is_approved` as the response variable and 
`is_female` as an explanatory variable since we are interested in the 
relationship between the two._

You can check that the multiverse `mv` defined by adding the formula branch has
4 different "universes" where each universe contains a unique combination of the
two covariates.

```{r}
nrow(summary(mv))
```

### Fitting the multiverse

`mverse` provides a set of functions that makes it easy to fit `glm()` across
the multiverse and extract the summary values.

For each of the model in the multiverse, we will

1. fit the logistic regression model using the dataset `hdma`,  
2. extract the coefficient estimate and 95% confidence interval for `is_female`, 
and
3. store the extracted estimate and confidence interval.

__Run__ each of the code chunks below to define the multiverse with the package.

1. Specify the underlying distribution family using `family_branch(binomial)`. 
This step sets `family = binomial` for all universes when calling `glm()` in the
next step.

```{r}
binom_family <- family_branch(binomial)
mv <- add_family_branch(mv, binom_family)
```

2. Fit the logistic regression models using `glm_mverse()`. The function fits
a logistic regression model for each universe in the multiverse by calling 
`glm()` as specified by `formula_branch()` and `family_branch()`.

```{r}
glm_mverse(mv)
```

_Note: Using the pipe operator, you can define a multiverse, add branches, and
fit models at once._

```{r}
mv <- create_multiverse(hdma) |>
  add_formula_branch(formulae) |>
  add_family_branch(binom_family) |>
  glm_mverse()
```

3. Extract and store summary values associated with `is_female`. 
`spec_summary()` allows you to extract the regression coefficient, p-values, 
95% confidence intervals, and other statistics associated with the term from
each regression model.

```{r}
results <- spec_summary(mv, "is_female")
results
```


### Exploring the multiverse

In this tutorial example, we only considered decisions around inclusion and 
exclusion of 2 out of 7 covariates. As you will see in the following activity,
a multiverse analysis can be complex and large. It is therefore important to
organize and represent the results in a human-readable format to help answering 
the research question.

First, we will present the result in a table using the `mverse` package's 
`spec_summary()`.

__Run__ the following code. We can use square bracket extractor to select the 
columns we want to display in order. `estimate`, `conft.low`, and `conf.high`
are the coefficient estimate, lower bound of the confidence interval, and upper
bound of the confidence interval respectively. You can display the covariate
specifications using `covariate_*_branch` columns where `*` is the name of a
covariate defined using `fomula_branch()`.

```{r}
results[ , c("covariate_payment_income_ratio_branch",
             "covariate_is_married_branch",
             "estimate", "conf.low", "conf.high")]
```

We now have a table that presents the results from the multiverse analysis in a
human-readable manner. From the table, we can see that all 4 analyses resulted
in 95% confidence intervals that contain 0. We can also see from the table that
including `is_married` as the covariate alone resulted in the largest
coefficient estimate.

Visualizing the table can help explore and deduce conclusions from the 
multiverse analysis. `mverse` includes functions that allow exploration of the
multiverse visually.

__Run__ the following code. `spec_curve()` from `mverse` plots the  coefficient 
estimates and the confidence intervals using outputs from `spec_summary()`. By
default, the function also uses colours to distinguish statistically significant
results at 95% confidence level.

```{r}
spec_curve(results)
```


From the plot above, we can tell that while all 4 models resulted in positive
coefficient estimates, the 95% confidence intervals all contain 0. The function
also plots the analyses sorted by the size of the coefficient estimates. The 
visual table at the bottom indicates to which model option each result belongs.
Here, `formulae` and `binom_family` branches have only 1 option each but the
covariate branches for `payment_income_ratio` and `is_married` each have the
option to include or exclude the covariate. By default the short form names are
displayed. We can also display the codes used for each option.

__Run__ the code below. `label = "code"` option in `spec_curve()` displays the 
code used to define the branch options instead of the short form names.

```{r}
spec_curve(results, label = "code")
```

Looking at the plot, it's clear that including `is_married` in the regression 
model resulted in higher coefficient estimates for `is_female`. To highlight the
comparison between models with and without `is_married`, we can visually group
the values by whether the model includes `is_married`.

__Run__ the code below. `spec_curve()`'s `colour_by` option allows colouring the
points and segments by arbitrary columns. We can create a new column based on 
the value of `covariate_is_married_branch` to distinguish results from models
that include `is_married`. 

```{r}
results["includes_is_married"] <- (
  results$covariate_is_married_branch == "include_is_married")
spec_curve(results, colour_by = "includes_is_married")
```

_Note: Using `covariate_is_married_branch` directly results in an error since
the column is used internally to construct the table at the bottom._

Suppose you want to highlight the model that includes both `is_married` and
`payment_income_ratio`. You can use `&` to combine more than one logical checks
to check that all of them are true. `order_by = "has_both"` in `spec_curve()` 
sorts the values so that the model with both covariates are appear on the right.

```{r}
results["has_both"] <- (
  results$covariate_is_married_branch == "include_is_married") & (
    results$covariate_payment_income_ratio_branch == 
      "include_payment_income_ratio"
  )
spec_curve(results, colour_by = "has_both", order_by = "has_both")
```

Alternatively, you can make use of `ggplot2` library's functions because 
`spec_curve()`'s output is a `ggplot` object. For example, you can use 
`facet_grid()` to isolate those with both covariates in different a different
row as shown below. This method allows you to keep the colour scheme for 
statistical significance.

```{r}
library(ggplot2)
spec_curve(results) +
  facet_grid(rows = vars(has_both))
```

_Note: Using column facets with `spec_curve()` is not recommended as repeating 
the table at the bottom causes overlaps._

> In all 4 models fitted in the multiverse analysis, we do not find 
statistically significant evidence that an applicant's sex was relevant in the 
probability of approval for their mortgage application. The multiverse analysis
further strengthened our conclusion from the simple model with no covariates.
However, it's worth noting that including the information about an applicant's
marital status visibly increased the estimated effect size. 

Indeed, you will see that the larger multiverse analysis in the following 
activity consists of results that are both statistically significant and not.

> Knit the current document by clicking `knit` button at the top ro pressing
`Shift + Ctrl + K`. Proceed back to the Quercus quiz. For the following 
activity, keep the rendered `tutorial.html` document open for reference.
